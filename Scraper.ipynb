{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tool\n",
    "\n",
    "The code below includes comments wherever possible. For reasons of reproducibility, the final data dictionary is stored in a `.pkl` format. If a different selection of restaurants is desired, simply delete the `top3restaurants.pkl` file from the same directory and execute this notebook again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setting up Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utkar\\.conda\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\utkar\\.conda\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "c:\\Users\\utkar\\.conda\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import time # for timekeeping\n",
    "toolStart = time.time()\n",
    "from selenium import webdriver # used here for automated operation of the website, and to scrape content by element-by-element\n",
    "from bs4 import BeautifulSoup # used here to scrape entire divs/elements, and extract information from their sub-elements\n",
    "import re # Regular expression matching\n",
    "import pandas as pd # Dataframe/.csv manipulation\n",
    "import pickle # saving objects into files for later use and reproducibility\n",
    "\n",
    "\n",
    "# standard headers to prevent restrictions from Swiggy's end\n",
    "headers = {\n",
    "    'Access-Control-Allow-Origin': '*',\n",
    "    'Access-Control-Allow-Methods': 'GET',\n",
    "    'Access-Control-Allow-Headers': 'Content-Type',\n",
    "    'Access-Control-Max-Age': '3600',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }\n",
    "    \n",
    "# initialising Chrome webdriver\n",
    "browser = webdriver.Chrome(executable_path='./chromedriver_win32/chromedriver.exe')\n",
    "browser.maximize_window()\n",
    "\n",
    "# page containing all restaurant listings in Bangalore\n",
    "listingPage = 'https://www.swiggy.com/city/bangalore'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Selecting Restaurants from Swiggy's Bangalore page\n",
    "\n",
    "The method employed here will pick the top 3 restaurants (w.r.t. star rating) on the front page of Swiggy's Bangalore page. This ensures that if the ratings on the front page change, the top 3 restaurants will be selected regardless.\n",
    "\n",
    "*For context, the top restaurants on the frontpage changed **five times** over the course of testing this scraper.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 16 frontpage restaurant links, out of which the top 3 are: \n",
      " [('https://www.swiggy.com/restaurants/kwality-walls-frozen-dessert-and-ice-cream-shop-btm-btm-2nd-stage-bangalore-298068', 4.3), ('https://www.swiggy.com/restaurants/craving-o-clock-btm-btm-2nd-stage-bangalore-362852', 4.3), ('https://www.swiggy.com/restaurants/chai-point-doddakannelli-villaymma-layouts-bangalore-286575', 4.3)]\n"
     ]
    }
   ],
   "source": [
    "browser.get(listingPage)\n",
    "links = browser.find_elements_by_class_name('_1j_Yo')\n",
    "ratings = []\n",
    "for i in browser.find_elements_by_class_name('_3Mn31'):\n",
    "    stars = i.text.split()[0]\n",
    "    if stars == '--':\n",
    "        stars = 0 # star rating unavailable\n",
    "    else:\n",
    "        stars = float(stars)\n",
    "    ratings.append(stars)\n",
    "ratingDict = {}\n",
    "for link, rating in zip(links, ratings):\n",
    "    ratingDict[link.get_attribute('href')] = rating\n",
    "\n",
    "# Sorting ratingDict on the basis of values (i.e. ratings)\n",
    "top3Links = sorted(ratingDict.items(), reverse = True, key = lambda di:(di[1], di[0]))[:3] # lambda function represents tuple comparison\n",
    "print('Scraped %d frontpage restaurant links, out of which the top 3 are:'%(len(ratingDict)), '\\n', top3Links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scraping and Collating Menu Info for Top 3 Restaurants\n",
    "\n",
    "On Swiggy, the restaurant page already loads all menu items upon opening (i.e. no lazyloading). Thus, we don't need to simulate a scroll to the bottom. This also means we can extract all HTML content at once through `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menu organisation concluded with a total of 257 items scraped, taking only 7.28 seconds!\n",
      "Restaurants scraped:  Kwality Walls Frozen Dessert and Ice Cream Shop, Btm 2nd Stage | Craving O Clock, Btm 2nd Stage | Chai Point, Villaymma Layouts\n"
     ]
    }
   ],
   "source": [
    "restLinks = [i[0] for i in top3Links]\n",
    "\n",
    "def menuScrape(browser, link):\n",
    "    restData = [] # creating a list of dicts (menu items), so that DataFrame organisation is easier\n",
    "    browser.get(link)\n",
    "    name = browser.find_element_by_class_name('_3aqeL').text # restaurant name\n",
    "    area = browser.find_elements_by_class_name('_3duMr')[2].text # restaurant area\n",
    "    rating = browser.find_element_by_class_name('_2l3H5').text # restaurant star rating\n",
    "    numRev = browser.find_element_by_class_name('_1iYuU').text.split()[0] # number of restaurant reviews\n",
    "\n",
    "    pageSource = browser.page_source\n",
    "    pageSoup = BeautifulSoup(pageSource)\n",
    "    catDivs = pageSoup.find_all('div', class_='_2dS-v')\n",
    "\n",
    "    for catDiv in catDivs:\n",
    "        itemCat = catDiv.find('h2', class_=['M_o7R _27PKo', 'M_o7R']).get_text() # item category\n",
    "        if itemCat == \"Recommended\":\n",
    "            continue # ignoring 'Recommended' category wherever detected to reduce data redundancy\n",
    "        itemDivs = catDiv.find_all('div', class_='_2wg_t')\n",
    "\n",
    "        for itemDiv in itemDivs:\n",
    "            itemName = itemDiv.find('div', class_='styles_itemName__hLfgz').get_text()\n",
    "            itemPrice = itemDiv.find('span', class_='rupee').get_text()\n",
    "            desc = itemDiv.find('div', class_='styles_itemDesc__3vhM0')\n",
    "            itemDesc = desc.get_text() if desc else 'N.A.' # since some items don't have a description\n",
    "            tag = itemDiv.find('span', class_='styles_ribbon__3tZ21 styles_itemRibbon__353Fy')\n",
    "            itemTag = tag.get_text() if tag else 'NoTag'\n",
    "            itemBest = 1 if re.search('Bestseller', itemTag) else 0 # sets the tag to 1 if the item is a bestseller, otherwise 0\n",
    "            \n",
    "            # this dict will become a single row of the DataFrame we create in the next step. It contains all the required information\n",
    "            itemDict = {'restName': name, 'restArea': area, 'restRating': rating, 'restNumRev': numRev, \n",
    "                        'itemCat': itemCat, 'itemName': itemName, 'itemPrice': itemPrice, \n",
    "                        'itemDesc': itemDesc, 'itemBest': itemBest}\n",
    "            restData.append(itemDict)\n",
    "\n",
    "    return restData\n",
    "\n",
    "scrapeStart = time.time()\n",
    "# scraping the required information for all 3 restaurants\n",
    "itemData = []\n",
    "restaurants = []\n",
    "for restaurant in restLinks:\n",
    "    cache = menuScrape(browser, restaurant)\n",
    "    restId = cache[0]['restName'] + ', ' + cache[0]['restArea']\n",
    "    itemData.extend(cache)\n",
    "    restaurants.append(restId)\n",
    "cache = 0 # resetting cache in memory after execution\n",
    "scrapeEnd = time.time()\n",
    "\n",
    "print('Menu organisation concluded with a total of %d items scraped, taking only %.2f seconds!'%(len(itemData), scrapeEnd-scrapeStart))\n",
    "print('Restaurants scraped: ', restaurants[0], '|', restaurants[1], '|', restaurants[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exporting Data as .CSV\n",
    "\n",
    "This .csv is used for analysis in Tableau and Python later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scraping process took 15.06 seconds in total.\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(itemData).to_csv('top3_menudata.csv', index = False)\n",
    "browser.close() # closing automated browser window\n",
    "toolEnd = time.time()\n",
    "print('The scraping process took %.2f seconds in total.'%(toolEnd-toolStart))\n",
    "\n",
    "# saving data dict into a .pkl file for later retrieval. The if-conditional secures it against being overwritten.\n",
    "if os.path.exists('top3restaurants.pkl'):\n",
    "    pass\n",
    "else:\n",
    "    with open('top3restaurants.pkl', 'wb') as filePath:\n",
    "        pickle.dump(itemData, filePath)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cffa68df81deac2f0e4df978a05ca168f014e56156c5266c6b6f148e394d19a1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
